{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRIS Classification Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6qqAwrjkV1w"
      },
      "source": [
        "# ***IRIS Classification Prediction***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD-846VUlTFB"
      },
      "source": [
        "**Step 1: Importing the necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQ_yn0LPRPy"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev2fPtxalkEO"
      },
      "source": [
        "In this Classification problem- we are using Sequential model, and Dense layers are used. Other libraries utils which is used to convert the target variable into binary as this dataset contains multiclass classification, matplotlib to visualize, and preprocesing used to scale the values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk-eCpmYna3_"
      },
      "source": [
        "**Step 2: Importing the Dataset from sklearn library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlMxVphjPTN8"
      },
      "source": [
        "from sklearn.datasets import load_iris\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gtVE86nnk-9"
      },
      "source": [
        "Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZiOK8hXPXnq"
      },
      "source": [
        "df = load_iris()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5s4YBcbPdHw",
        "outputId": "7422fdb5-abf2-4b19-d90b-b82629b7c49f"
      },
      "source": [
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': '/usr/local/lib/python3.7/dist-packages/sklearn/datasets/data/iris.csv',\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1MkCPgPobXD"
      },
      "source": [
        "**Data contains four features - sepal length,sepal width,petal length,petal width in cm. Target variable contains three classes - setosa, versicolor, virginica.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U8qtx8hnufO"
      },
      "source": [
        "**Step 3: Spliting the data in to X and Y variables**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYnqyu4HPd1I"
      },
      "source": [
        "X = df[\"data\"]\n",
        "Y = df[\"target\"]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQmdKtYxpbsk"
      },
      "source": [
        "Since the values in the dataset are varying converting the values in the dataset into one scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "absKoVp8PgwA",
        "outputId": "3520fb23-891f-4f1f-cd8a-be8219126825"
      },
      "source": [
        "X = preprocessing.scale(X)\n",
        "X"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-9.00681170e-01,  1.01900435e+00, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.14301691e+00, -1.31979479e-01, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.38535265e+00,  3.28414053e-01, -1.39706395e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.50652052e+00,  9.82172869e-02, -1.28338910e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.02184904e+00,  1.24920112e+00, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-5.37177559e-01,  1.93979142e+00, -1.16971425e+00,\n",
              "        -1.05217993e+00],\n",
              "       [-1.50652052e+00,  7.88807586e-01, -1.34022653e+00,\n",
              "        -1.18381211e+00],\n",
              "       [-1.02184904e+00,  7.88807586e-01, -1.28338910e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.74885626e+00, -3.62176246e-01, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.14301691e+00,  9.82172869e-02, -1.28338910e+00,\n",
              "        -1.44707648e+00],\n",
              "       [-5.37177559e-01,  1.47939788e+00, -1.28338910e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.26418478e+00,  7.88807586e-01, -1.22655167e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.26418478e+00, -1.31979479e-01, -1.34022653e+00,\n",
              "        -1.44707648e+00],\n",
              "       [-1.87002413e+00, -1.31979479e-01, -1.51073881e+00,\n",
              "        -1.44707648e+00],\n",
              "       [-5.25060772e-02,  2.16998818e+00, -1.45390138e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.73673948e-01,  3.09077525e+00, -1.28338910e+00,\n",
              "        -1.05217993e+00],\n",
              "       [-5.37177559e-01,  1.93979142e+00, -1.39706395e+00,\n",
              "        -1.05217993e+00],\n",
              "       [-9.00681170e-01,  1.01900435e+00, -1.34022653e+00,\n",
              "        -1.18381211e+00],\n",
              "       [-1.73673948e-01,  1.70959465e+00, -1.16971425e+00,\n",
              "        -1.18381211e+00],\n",
              "       [-9.00681170e-01,  1.70959465e+00, -1.28338910e+00,\n",
              "        -1.18381211e+00],\n",
              "       [-5.37177559e-01,  7.88807586e-01, -1.16971425e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-9.00681170e-01,  1.47939788e+00, -1.28338910e+00,\n",
              "        -1.05217993e+00],\n",
              "       [-1.50652052e+00,  1.24920112e+00, -1.56757623e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-9.00681170e-01,  5.58610819e-01, -1.16971425e+00,\n",
              "        -9.20547742e-01],\n",
              "       [-1.26418478e+00,  7.88807586e-01, -1.05603939e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.02184904e+00, -1.31979479e-01, -1.22655167e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.02184904e+00,  7.88807586e-01, -1.22655167e+00,\n",
              "        -1.05217993e+00],\n",
              "       [-7.79513300e-01,  1.01900435e+00, -1.28338910e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-7.79513300e-01,  7.88807586e-01, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.38535265e+00,  3.28414053e-01, -1.22655167e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.26418478e+00,  9.82172869e-02, -1.22655167e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-5.37177559e-01,  7.88807586e-01, -1.28338910e+00,\n",
              "        -1.05217993e+00],\n",
              "       [-7.79513300e-01,  2.40018495e+00, -1.28338910e+00,\n",
              "        -1.44707648e+00],\n",
              "       [-4.16009689e-01,  2.63038172e+00, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.14301691e+00,  9.82172869e-02, -1.28338910e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.02184904e+00,  3.28414053e-01, -1.45390138e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-4.16009689e-01,  1.01900435e+00, -1.39706395e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.14301691e+00,  1.24920112e+00, -1.34022653e+00,\n",
              "        -1.44707648e+00],\n",
              "       [-1.74885626e+00, -1.31979479e-01, -1.39706395e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-9.00681170e-01,  7.88807586e-01, -1.28338910e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.02184904e+00,  1.01900435e+00, -1.39706395e+00,\n",
              "        -1.18381211e+00],\n",
              "       [-1.62768839e+00, -1.74335684e+00, -1.39706395e+00,\n",
              "        -1.18381211e+00],\n",
              "       [-1.74885626e+00,  3.28414053e-01, -1.39706395e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.02184904e+00,  1.01900435e+00, -1.22655167e+00,\n",
              "        -7.88915558e-01],\n",
              "       [-9.00681170e-01,  1.70959465e+00, -1.05603939e+00,\n",
              "        -1.05217993e+00],\n",
              "       [-1.26418478e+00, -1.31979479e-01, -1.34022653e+00,\n",
              "        -1.18381211e+00],\n",
              "       [-9.00681170e-01,  1.70959465e+00, -1.22655167e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.50652052e+00,  3.28414053e-01, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-6.58345429e-01,  1.47939788e+00, -1.28338910e+00,\n",
              "        -1.31544430e+00],\n",
              "       [-1.02184904e+00,  5.58610819e-01, -1.34022653e+00,\n",
              "        -1.31544430e+00],\n",
              "       [ 1.40150837e+00,  3.28414053e-01,  5.35408562e-01,\n",
              "         2.64141916e-01],\n",
              "       [ 6.74501145e-01,  3.28414053e-01,  4.21733708e-01,\n",
              "         3.95774101e-01],\n",
              "       [ 1.28034050e+00,  9.82172869e-02,  6.49083415e-01,\n",
              "         3.95774101e-01],\n",
              "       [-4.16009689e-01, -1.74335684e+00,  1.37546573e-01,\n",
              "         1.32509732e-01],\n",
              "       [ 7.95669016e-01, -5.92373012e-01,  4.78571135e-01,\n",
              "         3.95774101e-01],\n",
              "       [-1.73673948e-01, -5.92373012e-01,  4.21733708e-01,\n",
              "         1.32509732e-01],\n",
              "       [ 5.53333275e-01,  5.58610819e-01,  5.35408562e-01,\n",
              "         5.27406285e-01],\n",
              "       [-1.14301691e+00, -1.51316008e+00, -2.60315415e-01,\n",
              "        -2.62386821e-01],\n",
              "       [ 9.16836886e-01, -3.62176246e-01,  4.78571135e-01,\n",
              "         1.32509732e-01],\n",
              "       [-7.79513300e-01, -8.22569778e-01,  8.07091462e-02,\n",
              "         2.64141916e-01],\n",
              "       [-1.02184904e+00, -2.43394714e+00, -1.46640561e-01,\n",
              "        -2.62386821e-01],\n",
              "       [ 6.86617933e-02, -1.31979479e-01,  2.51221427e-01,\n",
              "         3.95774101e-01],\n",
              "       [ 1.89829664e-01, -1.97355361e+00,  1.37546573e-01,\n",
              "        -2.62386821e-01],\n",
              "       [ 3.10997534e-01, -3.62176246e-01,  5.35408562e-01,\n",
              "         2.64141916e-01],\n",
              "       [-2.94841818e-01, -3.62176246e-01, -8.98031345e-02,\n",
              "         1.32509732e-01],\n",
              "       [ 1.03800476e+00,  9.82172869e-02,  3.64896281e-01,\n",
              "         2.64141916e-01],\n",
              "       [-2.94841818e-01, -1.31979479e-01,  4.21733708e-01,\n",
              "         3.95774101e-01],\n",
              "       [-5.25060772e-02, -8.22569778e-01,  1.94384000e-01,\n",
              "        -2.62386821e-01],\n",
              "       [ 4.32165405e-01, -1.97355361e+00,  4.21733708e-01,\n",
              "         3.95774101e-01],\n",
              "       [-2.94841818e-01, -1.28296331e+00,  8.07091462e-02,\n",
              "        -1.30754636e-01],\n",
              "       [ 6.86617933e-02,  3.28414053e-01,  5.92245988e-01,\n",
              "         7.90670654e-01],\n",
              "       [ 3.10997534e-01, -5.92373012e-01,  1.37546573e-01,\n",
              "         1.32509732e-01],\n",
              "       [ 5.53333275e-01, -1.28296331e+00,  6.49083415e-01,\n",
              "         3.95774101e-01],\n",
              "       [ 3.10997534e-01, -5.92373012e-01,  5.35408562e-01,\n",
              "         8.77547895e-04],\n",
              "       [ 6.74501145e-01, -3.62176246e-01,  3.08058854e-01,\n",
              "         1.32509732e-01],\n",
              "       [ 9.16836886e-01, -1.31979479e-01,  3.64896281e-01,\n",
              "         2.64141916e-01],\n",
              "       [ 1.15917263e+00, -5.92373012e-01,  5.92245988e-01,\n",
              "         2.64141916e-01],\n",
              "       [ 1.03800476e+00, -1.31979479e-01,  7.05920842e-01,\n",
              "         6.59038469e-01],\n",
              "       [ 1.89829664e-01, -3.62176246e-01,  4.21733708e-01,\n",
              "         3.95774101e-01],\n",
              "       [-1.73673948e-01, -1.05276654e+00, -1.46640561e-01,\n",
              "        -2.62386821e-01],\n",
              "       [-4.16009689e-01, -1.51316008e+00,  2.38717193e-02,\n",
              "        -1.30754636e-01],\n",
              "       [-4.16009689e-01, -1.51316008e+00, -3.29657076e-02,\n",
              "        -2.62386821e-01],\n",
              "       [-5.25060772e-02, -8.22569778e-01,  8.07091462e-02,\n",
              "         8.77547895e-04],\n",
              "       [ 1.89829664e-01, -8.22569778e-01,  7.62758269e-01,\n",
              "         5.27406285e-01],\n",
              "       [-5.37177559e-01, -1.31979479e-01,  4.21733708e-01,\n",
              "         3.95774101e-01],\n",
              "       [ 1.89829664e-01,  7.88807586e-01,  4.21733708e-01,\n",
              "         5.27406285e-01],\n",
              "       [ 1.03800476e+00,  9.82172869e-02,  5.35408562e-01,\n",
              "         3.95774101e-01],\n",
              "       [ 5.53333275e-01, -1.74335684e+00,  3.64896281e-01,\n",
              "         1.32509732e-01],\n",
              "       [-2.94841818e-01, -1.31979479e-01,  1.94384000e-01,\n",
              "         1.32509732e-01],\n",
              "       [-4.16009689e-01, -1.28296331e+00,  1.37546573e-01,\n",
              "         1.32509732e-01],\n",
              "       [-4.16009689e-01, -1.05276654e+00,  3.64896281e-01,\n",
              "         8.77547895e-04],\n",
              "       [ 3.10997534e-01, -1.31979479e-01,  4.78571135e-01,\n",
              "         2.64141916e-01],\n",
              "       [-5.25060772e-02, -1.05276654e+00,  1.37546573e-01,\n",
              "         8.77547895e-04],\n",
              "       [-1.02184904e+00, -1.74335684e+00, -2.60315415e-01,\n",
              "        -2.62386821e-01],\n",
              "       [-2.94841818e-01, -8.22569778e-01,  2.51221427e-01,\n",
              "         1.32509732e-01],\n",
              "       [-1.73673948e-01, -1.31979479e-01,  2.51221427e-01,\n",
              "         8.77547895e-04],\n",
              "       [-1.73673948e-01, -3.62176246e-01,  2.51221427e-01,\n",
              "         1.32509732e-01],\n",
              "       [ 4.32165405e-01, -3.62176246e-01,  3.08058854e-01,\n",
              "         1.32509732e-01],\n",
              "       [-9.00681170e-01, -1.28296331e+00, -4.30827696e-01,\n",
              "        -1.30754636e-01],\n",
              "       [-1.73673948e-01, -5.92373012e-01,  1.94384000e-01,\n",
              "         1.32509732e-01],\n",
              "       [ 5.53333275e-01,  5.58610819e-01,  1.27429511e+00,\n",
              "         1.71209594e+00],\n",
              "       [-5.25060772e-02, -8.22569778e-01,  7.62758269e-01,\n",
              "         9.22302838e-01],\n",
              "       [ 1.52267624e+00, -1.31979479e-01,  1.21745768e+00,\n",
              "         1.18556721e+00],\n",
              "       [ 5.53333275e-01, -3.62176246e-01,  1.04694540e+00,\n",
              "         7.90670654e-01],\n",
              "       [ 7.95669016e-01, -1.31979479e-01,  1.16062026e+00,\n",
              "         1.31719939e+00],\n",
              "       [ 2.12851559e+00, -1.31979479e-01,  1.61531967e+00,\n",
              "         1.18556721e+00],\n",
              "       [-1.14301691e+00, -1.28296331e+00,  4.21733708e-01,\n",
              "         6.59038469e-01],\n",
              "       [ 1.76501198e+00, -3.62176246e-01,  1.44480739e+00,\n",
              "         7.90670654e-01],\n",
              "       [ 1.03800476e+00, -1.28296331e+00,  1.16062026e+00,\n",
              "         7.90670654e-01],\n",
              "       [ 1.64384411e+00,  1.24920112e+00,  1.33113254e+00,\n",
              "         1.71209594e+00],\n",
              "       [ 7.95669016e-01,  3.28414053e-01,  7.62758269e-01,\n",
              "         1.05393502e+00],\n",
              "       [ 6.74501145e-01, -8.22569778e-01,  8.76433123e-01,\n",
              "         9.22302838e-01],\n",
              "       [ 1.15917263e+00, -1.31979479e-01,  9.90107977e-01,\n",
              "         1.18556721e+00],\n",
              "       [-1.73673948e-01, -1.28296331e+00,  7.05920842e-01,\n",
              "         1.05393502e+00],\n",
              "       [-5.25060772e-02, -5.92373012e-01,  7.62758269e-01,\n",
              "         1.58046376e+00],\n",
              "       [ 6.74501145e-01,  3.28414053e-01,  8.76433123e-01,\n",
              "         1.44883158e+00],\n",
              "       [ 7.95669016e-01, -1.31979479e-01,  9.90107977e-01,\n",
              "         7.90670654e-01],\n",
              "       [ 2.24968346e+00,  1.70959465e+00,  1.67215710e+00,\n",
              "         1.31719939e+00],\n",
              "       [ 2.24968346e+00, -1.05276654e+00,  1.78583195e+00,\n",
              "         1.44883158e+00],\n",
              "       [ 1.89829664e-01, -1.97355361e+00,  7.05920842e-01,\n",
              "         3.95774101e-01],\n",
              "       [ 1.28034050e+00,  3.28414053e-01,  1.10378283e+00,\n",
              "         1.44883158e+00],\n",
              "       [-2.94841818e-01, -5.92373012e-01,  6.49083415e-01,\n",
              "         1.05393502e+00],\n",
              "       [ 2.24968346e+00, -5.92373012e-01,  1.67215710e+00,\n",
              "         1.05393502e+00],\n",
              "       [ 5.53333275e-01, -8.22569778e-01,  6.49083415e-01,\n",
              "         7.90670654e-01],\n",
              "       [ 1.03800476e+00,  5.58610819e-01,  1.10378283e+00,\n",
              "         1.18556721e+00],\n",
              "       [ 1.64384411e+00,  3.28414053e-01,  1.27429511e+00,\n",
              "         7.90670654e-01],\n",
              "       [ 4.32165405e-01, -5.92373012e-01,  5.92245988e-01,\n",
              "         7.90670654e-01],\n",
              "       [ 3.10997534e-01, -1.31979479e-01,  6.49083415e-01,\n",
              "         7.90670654e-01],\n",
              "       [ 6.74501145e-01, -5.92373012e-01,  1.04694540e+00,\n",
              "         1.18556721e+00],\n",
              "       [ 1.64384411e+00, -1.31979479e-01,  1.16062026e+00,\n",
              "         5.27406285e-01],\n",
              "       [ 1.88617985e+00, -5.92373012e-01,  1.33113254e+00,\n",
              "         9.22302838e-01],\n",
              "       [ 2.49201920e+00,  1.70959465e+00,  1.50164482e+00,\n",
              "         1.05393502e+00],\n",
              "       [ 6.74501145e-01, -5.92373012e-01,  1.04694540e+00,\n",
              "         1.31719939e+00],\n",
              "       [ 5.53333275e-01, -5.92373012e-01,  7.62758269e-01,\n",
              "         3.95774101e-01],\n",
              "       [ 3.10997534e-01, -1.05276654e+00,  1.04694540e+00,\n",
              "         2.64141916e-01],\n",
              "       [ 2.24968346e+00, -1.31979479e-01,  1.33113254e+00,\n",
              "         1.44883158e+00],\n",
              "       [ 5.53333275e-01,  7.88807586e-01,  1.04694540e+00,\n",
              "         1.58046376e+00],\n",
              "       [ 6.74501145e-01,  9.82172869e-02,  9.90107977e-01,\n",
              "         7.90670654e-01],\n",
              "       [ 1.89829664e-01, -1.31979479e-01,  5.92245988e-01,\n",
              "         7.90670654e-01],\n",
              "       [ 1.28034050e+00,  9.82172869e-02,  9.33270550e-01,\n",
              "         1.18556721e+00],\n",
              "       [ 1.03800476e+00,  9.82172869e-02,  1.04694540e+00,\n",
              "         1.58046376e+00],\n",
              "       [ 1.28034050e+00,  9.82172869e-02,  7.62758269e-01,\n",
              "         1.44883158e+00],\n",
              "       [-5.25060772e-02, -8.22569778e-01,  7.62758269e-01,\n",
              "         9.22302838e-01],\n",
              "       [ 1.15917263e+00,  3.28414053e-01,  1.21745768e+00,\n",
              "         1.44883158e+00],\n",
              "       [ 1.03800476e+00,  5.58610819e-01,  1.10378283e+00,\n",
              "         1.71209594e+00],\n",
              "       [ 1.03800476e+00, -1.31979479e-01,  8.19595696e-01,\n",
              "         1.44883158e+00],\n",
              "       [ 5.53333275e-01, -1.28296331e+00,  7.05920842e-01,\n",
              "$        9.22302838e-01],\n",
              "       [ 7.95669016e-01, -1.31979479e-01,  8.19595696e-01,\n",
              "         1.05393502e+00],\n",
              "       [ 4.32165405e-01,  7.88807586e-01,  9.33270550e-01,\n",
              "         1.44883158e+00],\n",
              "       [ 6.86617933e-02, -1.31979479e-01,  7.62758269e-01,\n",
              "         7.90670654e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC0rKFbbp3zk"
      },
      "source": [
        "As this dataset is multiclass classification problem converting the target variable into binary since inorder for the Neural network to learn and predict it should be in binary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BbrK1KsPidA",        "outputId": "d4650b3b-dce3-4be3-84df-a17809ad588d"
      },
      "source": [
        "Y = to_categorical(Y)\n",
        "Y"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mAmwUUTqS82"
      },
      "source": [
        "**Step 4: Importing train and test model to split the data into train data and test data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bTFEsrVPlqY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXEuoa8YPodg"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state = 1234, stratify = Y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlHO1ZReqjDa"
      },
      "source": [
        "**Step 5: Creating the model for Neural Network and adding layers and functions to be used.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCzrPXmpPqQX"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(8,activation=\"relu\", input_dim = 4))\n",
        "model.add(Dense(3,activation=\"softmax\"))\n",
        "model.compile(loss = \"categorical_crossentropy\",optimizer = \"adam\",metrics = [\"accuracy\"])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuvGCtT4q3C0"
      },
      "source": [
        "1) We have used sequential model here. Dense layers are created and number of nodes created is 8, activation function used is relu as it mostly looks and acts like a linear activation function which easier for NN for computational."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VnebS14tqyr"
      },
      "source": [
        "2) This is the output layer here since there are only 3 classess 3 nodes are used and activation function used is Softmax. Since this a Multiclass problem, Softmax function is the best to identify the correct class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nBnP1ciuUVE"
      },
      "source": [
        "3) While compiling the paremeters used to compute loss function is Categorical_crossentropy due to multiclass, and optimiszer used is Adam as it is the perfect combination of momemtum and RMSprop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSFgZ-jlPvov",
        "outputId": "6a75f5bb-4269-4e11-c9f3-cb1bcf63d472"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 8)                 40        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 27        \n",
            "=================================================================\n",
            "Total params: 67\n",
            "Trainable params: 67\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxPllcgSwg4F"
      },
      "source": [
        "The trainable param of layer 1 is 40 as there are (4 inputs * 8 nodes)+8 biases and for layer 2 is 27 as there are (8 nodes in first layer * 3 node in output) + 3 biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD_JRVl5PwQG",
        "outputId": "dc0c38db-12c2-4584-9b04-e375cab637e6"
      },
      "source": [
        "print(model.get_weights())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.24103293,  0.03886175,  0.07668078,  0.21630388,  0.18182033,\n",
            "        -0.63798714, -0.37010816,  0.12852865],\n",
            "       [ 0.682102  , -0.02189696, -0.66389734, -0.60015124,  0.1070289 ,\n",
            "         0.09161192,  0.2509275 ,  0.40216678],\n",
            "       [-0.03956729,  0.6027053 ,  0.5351004 ,  0.36774367, -0.68702495,\n",
            "         0.4742928 ,  0.33418745, -0.15224755],\n",
            "       [-0.32057905,  0.16035473, -0.5479741 , -0.29525974,  0.4506678 ,\n",
            "         0.45253712, -0.41210395, -0.67817193]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.07243878, -0.62608933, -0.54917943],\n",
            "       [ 0.20046014, -0.30870253,  0.5433567 ],\n",
            "       [-0.17814028, -0.2337634 ,  0.59674126],\n",
            "       [ 0.33870143,  0.5299023 ,  0.1436634 ],\n",
            "       [-0.24481598, -0.669338  , -0.2895198 ],\n",
            "       [-0.4023289 , -0.5013245 , -0.32369712],\n",
            "       [ 0.22318852, -0.38510916, -0.73639315],\n",
            "       [-0.5448328 , -0.6929709 , -0.2849888 ]], dtype=float32), array([0., 0., 0.], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPo98ObyxeKI"
      },
      "source": [
        "**The above are Weights and biases initialized for inputs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXHh60Lpxt7_"
      },
      "source": [
        "**Step 6: training the model initialy with 100 epochs and batchsize 10**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX7J7-mVPyxw",
        "outputId": "c9291246-2629-4a5b-ba0f-e14582559af0"
      },
      "source": [
        "NN_model= model.fit(x_train,y_train, validation_data=(x_test,y_test),epochs=100,batch_size=10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 48ms/step - loss: 0.8583 - accuracy: 0.5889 - val_loss: 0.8190 - val_accuracy: 0.6667\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8022 - accuracy: 0.6257 - val_loss: 0.7910 - val_accuracy: 0.6667\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7694 - accuracy: 0.6465 - val_loss: 0.7654 - val_accuracy: 0.6667\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6763 - accuracy: 0.7117 - val_loss: 0.7410 - val_accuracy: 0.6667\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6774 - accuracy: 0.6725 - val_loss: 0.7189 - val_accuracy: 0.6667\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6562 - accuracy: 0.6804 - val_loss: 0.6979 - val_accuracy: 0.6667\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6600 - accuracy: 0.6498 - val_loss: 0.6781 - val_accuracy: 0.6667\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6078 - accuracy: 0.7003 - val_loss: 0.6594 - val_accuracy: 0.6667\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6064 - accuracy: 0.7050 - val_loss: 0.6417 - val_accuracy: 0.7000\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5702 - accuracy: 0.8171 - val_loss: 0.6253 - val_accuracy: 0.7333\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5198 - accuracy: 0.8202 - val_loss: 0.6096 - val_accuracy: 0.7333\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.8780 - val_loss: 0.5952 - val_accuracy: 0.7333\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5048 - accuracy: 0.8153 - val_loss: 0.5808 - val_accuracy: 0.7333\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5035 - accuracy: 0.8336 - val_loss: 0.5673 - val_accuracy: 0.7333\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4623 - accuracy: 0.8788 - val_loss: 0.5545 - val_accuracy: 0.7333\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4735 - accuracy: 0.8070 - val_loss: 0.5426 - val_accuracy: 0.7333\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5231 - accuracy: 0.8095 - val_loss: 0.5311 - val_accuracy: 0.7333\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3903 - accuracy: 0.8980 - val_loss: 0.5210 - val_accuracy: 0.7333\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4353 - accuracy: 0.8432 - val_loss: 0.5111 - val_accuracy: 0.7333\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3818 - accuracy: 0.9073 - val_loss: 0.5017 - val_accuracy: 0.7333\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3991 - accuracy: 0.8950 - val_loss: 0.4929 - val_accuracy: 0.7000\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4273 - accuracy: 0.8659 - val_loss: 0.4842 - val_accuracy: 0.7000\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3909 - accuracy: 0.8925 - val_loss: 0.4762 - val_accuracy: 0.7000\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4154 - accuracy: 0.8757 - val_loss: 0.4685 - val_accuracy: 0.7000\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3700 - accuracy: 0.8664 - val_loss: 0.4617 - val_accuracy: 0.7333\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3897 - accuracy: 0.8645 - val_loss: 0.4548 - val_accuracy: 0.7667\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3723 - accuracy: 0.8516 - val_loss: 0.4486 - val_accuracy: 0.7667\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3263 - accuracy: 0.9039 - val_loss: 0.4429 - val_accuracy: 0.7667\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3702 - accuracy: 0.8108 - val_loss: 0.4372 - val_accuracy: 0.8000\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3480 - accuracy: 0.8642 - val_loss: 0.4318 - val_accuracy: 0.8333\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3407 - accuracy: 0.8915 - val_loss: 0.4269 - val_accuracy: 0.8333\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2746 - accuracy: 0.9356 - val_loss: 0.4222 - val_accuracy: 0.8333\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2711 - accuracy: 0.9088 - val_loss: 0.4177 - val_accuracy: 0.8333\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2936 - accuracy: 0.9101 - val_loss: 0.4133 - val_accuracy: 0.8333\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2753 - accuracy: 0.8992 - val_loss: 0.4090 - val_accuracy: 0.8333\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3144 - accuracy: 0.8992 - val_loss: 0.4049 - val_accuracy: 0.8333\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3009 - accuracy: 0.9106 - val_loss: 0.4010 - val_accuracy: 0.8333\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3109 - accuracy: 0.8987 - val_loss: 0.3972 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3322 - accuracy: 0.8890 - val_loss: 0.3935 - val_accuracy: 0.8333\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.9070 - val_loss: 0.3902 - val_accuracy: 0.8667\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2648 - accuracy: 0.9123 - val_loss: 0.3869 - val_accuracy: 0.8667\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2463 - accuracy: 0.9015 - val_loss: 0.3839 - val_accuracy: 0.8667\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2901 - accuracy: 0.9063 - val_loss: 0.3805 - val_accuracy: 0.8667\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2551 - accuracy: 0.9202 - val_loss: 0.3775 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2776 - accuracy: 0.9053 - val_loss: 0.3744 - val_accuracy: 0.8667\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2389 - accuracy: 0.9284 - val_loss: 0.3717 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2129 - accuracy: 0.9423 - val_loss: 0.3690 - val_accuracy: 0.8667\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2433 - accuracy: 0.9018 - val_loss: 0.3660 - val_accuracy: 0.8667\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2220 - accuracy: 0.9182 - val_loss: 0.3633 - val_accuracy: 0.8667\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2018 - accuracy: 0.9484 - val_loss: 0.3607 - val_accuracy: 0.8667\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2099 - accuracy: 0.9220 - val_loss: 0.3582 - val_accuracy: 0.8667\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2179 - accuracy: 0.9202 - val_loss: 0.3555 - val_accuracy: 0.8667\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2235 - accuracy: 0.9120 - val_loss: 0.3528 - val_accuracy: 0.8667\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1708 - accuracy: 0.9326 - val_loss: 0.3506 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2596 - accuracy: 0.8714 - val_loss: 0.3479 - val_accuracy: 0.9000\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1981 - accuracy: 0.9108 - val_loss: 0.3456 - val_accuracy: 0.8667\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1899 - accuracy: 0.9639 - val_loss: 0.3432 - val_accuracy: 0.8667\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.9601 - val_loss: 0.3410 - val_accuracy: 0.8667\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2120 - accuracy: 0.9102 - val_loss: 0.3388 - val_accuracy: 0.9000\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1673 - accuracy: 0.9555 - val_loss: 0.3367 - val_accuracy: 0.9000\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1700 - accuracy: 0.9528 - val_loss: 0.3342 - val_accuracy: 0.9000\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2099 - accuracy: 0.9018 - val_loss: 0.3319 - val_accuracy: 0.9000\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2103 - accuracy: 0.9137 - val_loss: 0.3294 - val_accuracy: 0.9000\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1767 - accuracy: 0.9348 - val_loss: 0.3272 - val_accuracy: 0.9000\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1841 - accuracy: 0.9525 - val_loss: 0.3249 - val_accuracy: 0.9000\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1869 - accuracy: 0.9534 - val_loss: 0.3224 - val_accuracy: 0.9000\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1781 - accuracy: 0.9553 - val_loss: 0.3201 - val_accuracy: 0.9000\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1574 - accuracy: 0.9536 - val_loss: 0.3179 - val_accuracy: 0.9000\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1862 - accuracy: 0.9143 - val_loss: 0.3155 - val_accuracy: 0.9000\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.9659 - val_loss: 0.3135 - val_accuracy: 0.9000\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9803 - val_loss: 0.3114 - val_accuracy: 0.9000\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1849 - accuracy: 0.9511 - val_loss: 0.3090 - val_accuracy: 0.9000\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.9428 - val_loss: 0.3068 - val_accuracy: 0.9000\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1535 - accuracy: 0.9696 - val_loss: 0.3048 - val_accuracy: 0.9000\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1485 - accuracy: 0.9620 - val_loss: 0.3030 - val_accuracy: 0.9000\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9783 - val_loss: 0.3006 - val_accuracy: 0.9000\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9713 - val_loss: 0.2984 - val_accuracy: 0.9000\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1281 - accuracy: 0.9858 - val_loss: 0.2968 - val_accuracy: 0.9000\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1673 - accuracy: 0.9757 - val_loss: 0.2945 - val_accuracy: 0.9000\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1441 - accuracy: 0.9646 - val_loss: 0.2926 - val_accuracy: 0.9000\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1579 - accuracy: 0.9665 - val_loss: 0.2906 - val_accuracy: 0.9000\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9748 - val_loss: 0.2886 - val_accuracy: 0.9000\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1673 - accuracy: 0.9647 - val_loss: 0.2868 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1411 - accuracy: 0.9588 - val_loss: 0.2849 - val_accuracy: 0.9000\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1335 - accuracy: 0.9782 - val_loss: 0.2831 - val_accuracy: 0.9000\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9618 - val_loss: 0.2811 - val_accuracy: 0.9000\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1237 - accuracy: 0.9841 - val_loss: 0.2792 - val_accuracy: 0.9000\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9726 - val_loss: 0.2772 - val_accuracy: 0.9000\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9766 - val_loss: 0.2756 - val_accuracy: 0.9000\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1490 - accuracy: 0.9562 - val_loss: 0.2734 - val_accuracy: 0.9000\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9855 - val_loss: 0.2717 - val_accuracy: 0.9000\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9794 - val_loss: 0.2700 - val_accuracy: 0.9000\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1481 - accuracy: 0.9732 - val_loss: 0.2680 - val_accuracy: 0.9000\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1499 - accuracy: 0.9577 - val_loss: 0.2661 - val_accuracy: 0.9000\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9636 - val_loss: 0.2643 - val_accuracy: 0.9000\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9729 - val_loss: 0.2631 - val_accuracy: 0.9000\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9838 - val_loss: 0.2615 - val_accuracy: 0.9000\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1406 - accuracy: 0.9628 - val_loss: 0.2593 - val_accuracy: 0.9000\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1144 - accuracy: 0.9750 - val_loss: 0.2578 - val_accuracy: 0.9000\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9832 - val_loss: 0.2563 - val_accuracy: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhLhKAxKyEch"
      },
      "source": [
        "The train accuracy is 98% but the test accuracy is 90%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqJG2myOP7sR",
        "outputId": "2029a27e-f872-452b-d754-1fe2602700c6"
      },
      "source": [
        "score = model.evaluate(x_test,y_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 118ms/step - loss: 0.2563 - accuracy: 0.9000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9MmO-16P8q_",
        "outputId": "f3a3d6ef-6b68-45bb-88bc-1d6ee3c2d45c"
      },
      "source": [
        "print(model.get_weights())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.554015  ,  0.01132559,  0.28129426,  0.3058374 , -0.10596034,\n",
            "        -0.44731385, -0.60503596, -0.11547276],\n",
            "       [ 0.6291384 , -0.1513927 , -0.6709831 , -0.9761317 ,  0.2186148 ,\n",
            "        -0.0554883 ,  0.37288085,  0.34183204],\n",
            "       [-0.38690072,  0.9872484 ,  0.9968222 ,  0.30477697, -1.0961049 ,\n",
            "         1.0722985 ,  0.0697052 , -0.51719356],\n",
            "       [-0.67424923,  0.6728135 , -0.02659202, -0.68282175,  0.00142933,\n",
            "         1.1416222 , -0.6951663 , -1.0637186 ]], dtype=float32), array([ 0.11792193, -0.3688422 ,  0.03695225,  0.7723625 ,  0.05553498,\n",
            "       -0.19753213,  0.2764129 ,  0.09428015], dtype=float32), array([[ 0.34254307, -0.9034739 , -0.8037697 ],\n",
            "       [-0.21399851, -0.6364194 ,  1.0505859 ],\n",
            "       [-0.7573943 ,  0.04915849,  0.60433716],\n",
            "       [-0.259653  ,  1.1167681 , -0.29426774],\n",
            "       [ 0.20869362, -1.1201767 , -0.6684562 ],\n",
            "       [-0.9824073 , -0.80337924,  0.28638467],\n",
            "       [ 0.50392926, -0.44268087, -1.1681186 ],\n",
            "       [-0.24660745, -1.0832136 , -0.5029223 ]], dtype=float32), array([-0.53222495,  0.58392924, -0.40715256], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHZsKZ_MyN6H"
      },
      "source": [
        "The above weight and biases are changed when the models are trained and adjusted the weight and biases calculating the loss by backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_5t-WxRyjPn"
      },
      "source": [
        "**Step 7: Now adjusting the Epochs to 300 with same batch size**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwNC8jL6zm0d"
      },
      "source": [
        "The model is defined to NN_model_2 because in futher process it is used to retrive values for each epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kPk2-xPP-13",
        "outputId": "41e5b8a5-54b2-48a2-d9c2-2f75b9f20127"
      },
      "source": [
        "NN_model_2= model.fit(x_train,y_train, validation_data=(x_test,y_test),epochs=300,batch_size=10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1226 - accuracy: 0.9667 - val_loss: 0.2544 - val_accuracy: 0.9000\n",
            "Epoch 2/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9667 - val_loss: 0.2528 - val_accuracy: 0.9000\n",
            "Epoch 3/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1200 - accuracy: 0.9667 - val_loss: 0.2511 - val_accuracy: 0.9000\n",
            "Epoch 4/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9667 - val_loss: 0.2494 - val_accuracy: 0.9000\n",
            "Epoch 5/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9667 - val_loss: 0.2478 - val_accuracy: 0.9000\n",
            "Epoch 6/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9667 - val_loss: 0.2465 - val_accuracy: 0.9000\n",
            "Epoch 7/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1154 - accuracy: 0.9667 - val_loss: 0.2448 - val_accuracy: 0.9000\n",
            "Epoch 8/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1142 - accuracy: 0.9667 - val_loss: 0.2434 - val_accuracy: 0.9000\n",
            "Epoch 9/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9667 - val_loss: 0.2418 - val_accuracy: 0.9000\n",
            "Epoch 10/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9667 - val_loss: 0.2403 - val_accuracy: 0.9000\n",
            "Epoch 11/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9667 - val_loss: 0.2388 - val_accuracy: 0.9000\n",
            "Epoch 12/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1106 - accuracy: 0.9667 - val_loss: 0.2372 - val_accuracy: 0.9000\n",
            "Epoch 13/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1091 - accuracy: 0.9667 - val_loss: 0.2360 - val_accuracy: 0.9000\n",
            "Epoch 14/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1082 - accuracy: 0.9667 - val_loss: 0.2347 - val_accuracy: 0.9000\n",
            "Epoch 15/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9750 - val_loss: 0.2333 - val_accuracy: 0.9000\n",
            "Epoch 16/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1063 - accuracy: 0.9750 - val_loss: 0.2320 - val_accuracy: 0.9000\n",
            "Epoch 17/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1055 - accuracy: 0.9750 - val_loss: 0.2306 - val_accuracy: 0.9000\n",
            "Epoch 18/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1043 - accuracy: 0.9750 - val_loss: 0.2289 - val_accuracy: 0.9000\n",
            "Epoch 19/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.9750 - val_loss: 0.2275 - val_accuracy: 0.9000\n",
            "Epoch 20/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9750 - val_loss: 0.2261 - val_accuracy: 0.9000\n",
            "Epoch 21/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1016 - accuracy: 0.9750 - val_loss: 0.2249 - val_accuracy: 0.9000\n",
            "Epoch 22/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1009 - accuracy: 0.9750 - val_loss: 0.2240 - val_accuracy: 0.9000\n",
            "Epoch 23/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1000 - accuracy: 0.9750 - val_loss: 0.2223 - val_accuracy: 0.9000\n",
            "Epoch 24/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9750 - val_loss: 0.2212 - val_accuracy: 0.9000\n",
            "Epoch 25/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0982 - accuracy: 0.9750 - val_loss: 0.2198 - val_accuracy: 0.9000\n",
            "Epoch 26/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9750 - val_loss: 0.2189 - val_accuracy: 0.9000\n",
            "Epoch 27/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0966 - accuracy: 0.9750 - val_loss: 0.2175 - val_accuracy: 0.9000\n",
            "Epoch 28/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9750 - val_loss: 0.2163 - val_accuracy: 0.9000\n",
            "Epoch 29/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0953 - accuracy: 0.9750 - val_loss: 0.2147 - val_accuracy: 0.9000\n",
            "Epoch 30/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9750 - val_loss: 0.2138 - val_accuracy: 0.9000\n",
            "Epoch 31/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0938 - accuracy: 0.9750 - val_loss: 0.2129 - val_accuracy: 0.9333\n",
            "Epoch 32/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0928 - accuracy: 0.9750 - val_loss: 0.2114 - val_accuracy: 0.9333\n",
            "Epoch 33/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0920 - accuracy: 0.9750 - val_loss: 0.2103 - val_accuracy: 0.9333\n",
            "Epoch 34/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0914 - accuracy: 0.9750 - val_loss: 0.2091 - val_accuracy: 0.9333\n",
            "Epoch 35/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0906 - accuracy: 0.9750 - val_loss: 0.2082 - val_accuracy: 0.9333\n",
            "Epoch 36/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0898 - accuracy: 0.9750 - val_loss: 0.2071 - val_accuracy: 0.9333\n",
            "Epoch 37/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9750 - val_loss: 0.2060 - val_accuracy: 0.9333\n",
            "Epoch 38/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0885 - accuracy: 0.9750 - val_loss: 0.2046 - val_accuracy: 0.9333\n",
            "Epoch 39/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0878 - accuracy: 0.9750 - val_loss: 0.2036 - val_accuracy: 0.9333\n",
            "Epoch 40/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9750 - val_loss: 0.2025 - val_accuracy: 0.9333\n",
            "Epoch 41/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0865 - accuracy: 0.9750 - val_loss: 0.2014 - val_accuracy: 0.9333\n",
            "Epoch 42/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9750 - val_loss: 0.2002 - val_accuracy: 0.9333\n",
            "Epoch 43/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 0.9750 - val_loss: 0.1992 - val_accuracy: 0.9333\n",
            "Epoch 44/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0850 - accuracy: 0.9750 - val_loss: 0.1983 - val_accuracy: 0.9333\n",
            "Epoch 45/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0839 - accuracy: 0.9750 - val_loss: 0.1967 - val_accuracy: 0.9333\n",
            "Epoch 46/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0833 - accuracy: 0.9750 - val_loss: 0.1959 - val_accuracy: 0.9333\n",
            "Epoch 47/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.9750 - val_loss: 0.1949 - val_accuracy: 0.9333\n",
            "Epoch 48/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0822 - accuracy: 0.9750 - val_loss: 0.1933 - val_accuracy: 0.9333\n",
            "Epoch 49/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0817 - accuracy: 0.9750 - val_loss: 0.1925 - val_accuracy: 0.9333\n",
            "Epoch 50/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0811 - accuracy: 0.9750 - val_loss: 0.1911 - val_accuracy: 0.9333\n",
            "Epoch 51/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9750 - val_loss: 0.1897 - val_accuracy: 0.9333\n",
            "Epoch 52/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0799 - accuracy: 0.9750 - val_loss: 0.1885 - val_accuracy: 0.9333\n",
            "Epoch 53/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9750 - val_loss: 0.1874 - val_accuracy: 0.9333\n",
            "Epoch 54/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9750 - val_loss: 0.1863 - val_accuracy: 0.9333\n",
            "Epoch 55/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9750 - val_loss: 0.1849 - val_accuracy: 0.9333\n",
            "Epoch 56/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0776 - accuracy: 0.9750 - val_loss: 0.1836 - val_accuracy: 0.9333\n",
            "Epoch 57/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0773 - accuracy: 0.9750 - val_loss: 0.1824 - val_accuracy: 0.9333\n",
            "Epoch 58/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0766 - accuracy: 0.9750 - val_loss: 0.1812 - val_accuracy: 0.9333\n",
            "Epoch 59/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.9750 - val_loss: 0.1796 - val_accuracy: 0.9333\n",
            "Epoch 60/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0754 - accuracy: 0.9750 - val_loss: 0.1781 - val_accuracy: 0.9333\n",
            "Epoch 61/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9750 - val_loss: 0.1768 - val_accuracy: 0.9333\n",
            "Epoch 62/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9750 - val_loss: 0.1754 - val_accuracy: 0.9333\n",
            "Epoch 63/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9750 - val_loss: 0.1740 - val_accuracy: 0.9333\n",
            "Epoch 64/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0731 - accuracy: 0.9750 - val_loss: 0.1727 - val_accuracy: 0.9333\n",
            "Epoch 65/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0726 - accuracy: 0.9750 - val_loss: 0.1710 - val_accuracy: 0.9333\n",
            "Epoch 66/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0723 - accuracy: 0.9750 - val_loss: 0.1691 - val_accuracy: 0.9333\n",
            "Epoch 67/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9750 - val_loss: 0.1682 - val_accuracy: 0.9333\n",
            "Epoch 68/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9750 - val_loss: 0.1666 - val_accuracy: 0.9333\n",
            "Epoch 69/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0705 - accuracy: 0.9750 - val_loss: 0.1656 - val_accuracy: 0.9333\n",
            "Epoch 70/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 0.9750 - val_loss: 0.1640 - val_accuracy: 0.9333\n",
            "Epoch 71/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0695 - accuracy: 0.9750 - val_loss: 0.1630 - val_accuracy: 0.9333\n",
            "Epoch 72/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0690 - accuracy: 0.9750 - val_loss: 0.1618 - val_accuracy: 0.9333\n",
            "Epoch 73/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0697 - accuracy: 0.9750 - val_loss: 0.1605 - val_accuracy: 0.9333\n",
            "Epoch 74/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9750 - val_loss: 0.1591 - val_accuracy: 0.9333\n",
            "Epoch 75/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9750 - val_loss: 0.1582 - val_accuracy: 0.9333\n",
            "Epoch 76/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 0.1571 - val_accuracy: 0.9333\n",
            "Epoch 77/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0668 - accuracy: 0.9750 - val_loss: 0.1560 - val_accuracy: 0.9333\n",
            "Epoch 78/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0666 - accuracy: 0.9750 - val_loss: 0.1545 - val_accuracy: 0.9333\n",
            "Epoch 79/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9750 - val_loss: 0.1536 - val_accuracy: 0.9333\n",
            "Epoch 80/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0658 - accuracy: 0.9750 - val_loss: 0.1522 - val_accuracy: 0.9333\n",
            "Epoch 81/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 0.1518 - val_accuracy: 0.9333\n",
            "Epoch 82/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0648 - accuracy: 0.9750 - val_loss: 0.1507 - val_accuracy: 0.9333\n",
            "Epoch 83/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 0.9750 - val_loss: 0.1502 - val_accuracy: 0.9333\n",
            "Epoch 84/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0641 - accuracy: 0.9750 - val_loss: 0.1491 - val_accuracy: 0.9333\n",
            "Epoch 85/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0637 - accuracy: 0.9750 - val_loss: 0.1483 - val_accuracy: 0.9333\n",
            "Epoch 86/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9750 - val_loss: 0.1475 - val_accuracy: 0.9333\n",
            "Epoch 87/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0635 - accuracy: 0.9750 - val_loss: 0.1464 - val_accuracy: 0.9333\n",
            "Epoch 88/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9750 - val_loss: 0.1459 - val_accuracy: 0.9333\n",
            "Epoch 89/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9750 - val_loss: 0.1454 - val_accuracy: 0.9333\n",
            "Epoch 90/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9750 - val_loss: 0.1444 - val_accuracy: 0.9333\n",
            "Epoch 91/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.9750 - val_loss: 0.1439 - val_accuracy: 0.9333\n",
            "Epoch 92/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0614 - accuracy: 0.9750 - val_loss: 0.1431 - val_accuracy: 0.9333\n",
            "Epoch 93/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0612 - accuracy: 0.9750 - val_loss: 0.1423 - val_accuracy: 0.9333\n",
            "Epoch 94/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9750 - val_loss: 0.1421 - val_accuracy: 0.9333\n",
            "Epoch 95/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0605 - accuracy: 0.9750 - val_loss: 0.1411 - val_accuracy: 0.9333\n",
            "Epoch 96/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0602 - accuracy: 0.9750 - val_loss: 0.1406 - val_accuracy: 0.9333\n",
            "Epoch 97/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0605 - accuracy: 0.9750 - val_loss: 0.1393 - val_accuracy: 0.9667\n",
            "Epoch 98/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.9750 - val_loss: 0.1385 - val_accuracy: 0.9333\n",
            "Epoch 99/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.9750 - val_loss: 0.1385 - val_accuracy: 0.9333\n",
            "Epoch 100/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9750 - val_loss: 0.1380 - val_accuracy: 0.9333\n",
            "Epoch 101/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.1372 - val_accuracy: 0.9333\n",
            "Epoch 102/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0585 - accuracy: 0.9750 - val_loss: 0.1364 - val_accuracy: 0.9333\n",
            "Epoch 103/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 0.9750 - val_loss: 0.1358 - val_accuracy: 0.9333\n",
            "Epoch 104/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.1355 - val_accuracy: 0.9333\n",
            "Epoch 105/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0577 - accuracy: 0.9750 - val_loss: 0.1345 - val_accuracy: 0.9667\n",
            "Epoch 106/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 0.1341 - val_accuracy: 0.9667\n",
            "Epoch 107/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 0.1330 - val_accuracy: 0.9667\n",
            "Epoch 108/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9750 - val_loss: 0.1328 - val_accuracy: 0.9333\n",
            "Epoch 109/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0567 - accuracy: 0.9750 - val_loss: 0.1324 - val_accuracy: 0.9333\n",
            "Epoch 110/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0566 - accuracy: 0.9750 - val_loss: 0.1317 - val_accuracy: 0.9667\n",
            "Epoch 111/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.9750 - val_loss: 0.1312 - val_accuracy: 0.9667\n",
            "Epoch 112/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.1305 - val_accuracy: 0.9667\n",
            "Epoch 113/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9750 - val_loss: 0.1302 - val_accuracy: 0.9333\n",
            "Epoch 114/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.1297 - val_accuracy: 0.9667\n",
            "Epoch 115/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 0.9750 - val_loss: 0.1290 - val_accuracy: 0.9667\n",
            "Epoch 116/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.1287 - val_accuracy: 0.9667\n",
            "Epoch 117/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0547 - accuracy: 0.9750 - val_loss: 0.1281 - val_accuracy: 0.9667\n",
            "Epoch 118/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0546 - accuracy: 0.9750 - val_loss: 0.1280 - val_accuracy: 0.9667\n",
            "Epoch 119/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.1276 - val_accuracy: 0.9333\n",
            "Epoch 120/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9750 - val_loss: 0.1271 - val_accuracy: 0.9667\n",
            "Epoch 121/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.1274 - val_accuracy: 0.9333\n",
            "Epoch 122/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.1260 - val_accuracy: 0.9667\n",
            "Epoch 123/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0534 - accuracy: 0.9750 - val_loss: 0.1251 - val_accuracy: 0.9667\n",
            "Epoch 124/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0533 - accuracy: 0.9750 - val_loss: 0.1249 - val_accuracy: 0.9667\n",
            "Epoch 125/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0530 - accuracy: 0.9750 - val_loss: 0.1247 - val_accuracy: 0.9667\n",
            "Epoch 126/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0529 - accuracy: 0.9750 - val_loss: 0.1241 - val_accuracy: 0.9667\n",
            "Epoch 127/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0527 - accuracy: 0.9750 - val_loss: 0.1237 - val_accuracy: 0.9667\n",
            "Epoch 128/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0525 - accuracy: 0.9750 - val_loss: 0.1238 - val_accuracy: 0.9667\n",
            "Epoch 129/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0522 - accuracy: 0.9750 - val_loss: 0.1231 - val_accuracy: 0.9667\n",
            "Epoch 130/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0520 - accuracy: 0.9750 - val_loss: 0.1231 - val_accuracy: 0.9667\n",
            "Epoch 131/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0518 - accuracy: 0.9750 - val_loss: 0.1227 - val_accuracy: 0.9667\n",
            "Epoch 132/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0518 - accuracy: 0.9750 - val_loss: 0.1226 - val_accuracy: 0.9667\n",
            "Epoch 133/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0514 - accuracy: 0.9750 - val_loss: 0.1222 - val_accuracy: 0.9667\n",
            "Epoch 134/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.9750 - val_loss: 0.1219 - val_accuracy: 0.9667\n",
            "Epoch 135/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0511 - accuracy: 0.9750 - val_loss: 0.1214 - val_accuracy: 0.9667\n",
            "Epoch 136/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0509 - accuracy: 0.9750 - val_loss: 0.1208 - val_accuracy: 0.9667\n",
            "Epoch 137/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.9750 - val_loss: 0.1211 - val_accuracy: 0.9667\n",
            "Epoch 138/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9750 - val_loss: 0.1201 - val_accuracy: 0.9667\n",
            "Epoch 139/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0504 - accuracy: 0.9750 - val_loss: 0.1204 - val_accuracy: 0.9667\n",
            "Epoch 140/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0503 - accuracy: 0.9750 - val_loss: 0.1207 - val_accuracy: 0.9667\n",
            "Epoch 141/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0500 - accuracy: 0.9750 - val_loss: 0.1201 - val_accuracy: 0.9667\n",
            "Epoch 142/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.9750 - val_loss: 0.1194 - val_accuracy: 0.9667\n",
            "Epoch 143/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0497 - accuracy: 0.9750 - val_loss: 0.1192 - val_accuracy: 0.9667\n",
            "Epoch 144/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0495 - accuracy: 0.9750 - val_loss: 0.1189 - val_accuracy: 0.9667\n",
            "Epoch 145/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0494 - accuracy: 0.9750 - val_loss: 0.1188 - val_accuracy: 0.9667\n",
            "Epoch 146/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0494 - accuracy: 0.9750 - val_loss: 0.1188 - val_accuracy: 0.9333\n",
            "Epoch 147/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0489 - accuracy: 0.9750 - val_loss: 0.1186 - val_accuracy: 0.9667\n",
            "Epoch 148/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9750 - val_loss: 0.1179 - val_accuracy: 0.9667\n",
            "Epoch 149/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0488 - accuracy: 0.9750 - val_loss: 0.1182 - val_accuracy: 0.9667\n",
            "Epoch 150/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.9750 - val_loss: 0.1176 - val_accuracy: 0.9667\n",
            "Epoch 151/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0485 - accuracy: 0.9750 - val_loss: 0.1177 - val_accuracy: 0.9333\n",
            "Epoch 152/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 0.9750 - val_loss: 0.1171 - val_accuracy: 0.9667\n",
            "Epoch 153/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9750 - val_loss: 0.1172 - val_accuracy: 0.9667\n",
            "Epoch 154/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0480 - accuracy: 0.9750 - val_loss: 0.1173 - val_accuracy: 0.9667\n",
            "Epoch 155/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0479 - accuracy: 0.9750 - val_loss: 0.1167 - val_accuracy: 0.9667\n",
            "Epoch 156/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.9750 - val_loss: 0.1166 - val_accuracy: 0.9667\n",
            "Epoch 157/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.9750 - val_loss: 0.1161 - val_accuracy: 0.9667\n",
            "Epoch 158/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.9750 - val_loss: 0.1162 - val_accuracy: 0.9333\n",
            "Epoch 159/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0472 - accuracy: 0.9750 - val_loss: 0.1159 - val_accuracy: 0.9667\n",
            "Epoch 160/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0474 - accuracy: 0.9750 - val_loss: 0.1153 - val_accuracy: 0.9667\n",
            "Epoch 161/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9750 - val_loss: 0.1153 - val_accuracy: 0.9667\n",
            "Epoch 162/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0468 - accuracy: 0.9750 - val_loss: 0.1156 - val_accuracy: 0.9667\n",
            "Epoch 163/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0467 - accuracy: 0.9750 - val_loss: 0.1151 - val_accuracy: 0.9667\n",
            "Epoch 164/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.9750 - val_loss: 0.1155 - val_accuracy: 0.9333\n",
            "Epoch 165/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9750 - val_loss: 0.1144 - val_accuracy: 0.9667\n",
            "Epoch 166/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.9750 - val_loss: 0.1148 - val_accuracy: 0.9333\n",
            "Epoch 167/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0465 - accuracy: 0.9750 - val_loss: 0.1144 - val_accuracy: 0.9667\n",
            "Epoch 168/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0467 - accuracy: 0.9750 - val_loss: 0.1152 - val_accuracy: 0.9333\n",
            "Epoch 169/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0459 - accuracy: 0.9750 - val_loss: 0.1145 - val_accuracy: 0.9333\n",
            "Epoch 170/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0458 - accuracy: 0.9750 - val_loss: 0.1142 - val_accuracy: 0.9333\n",
            "Epoch 171/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.9750 - val_loss: 0.1144 - val_accuracy: 0.9333\n",
            "Epoch 172/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.9750 - val_loss: 0.1135 - val_accuracy: 0.9667\n",
            "Epoch 173/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.9750 - val_loss: 0.1132 - val_accuracy: 0.9667\n",
            "Epoch 174/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0454 - accuracy: 0.9750 - val_loss: 0.1132 - val_accuracy: 0.9667\n",
            "Epoch 175/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0457 - accuracy: 0.9750 - val_loss: 0.1140 - val_accuracy: 0.9333\n",
            "Epoch 176/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0453 - accuracy: 0.9750 - val_loss: 0.1134 - val_accuracy: 0.9333\n",
            "Epoch 177/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0452 - accuracy: 0.9750 - val_loss: 0.1134 - val_accuracy: 0.9333\n",
            "Epoch 178/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0451 - accuracy: 0.9750 - val_loss: 0.1124 - val_accuracy: 0.9333\n",
            "Epoch 179/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0450 - accuracy: 0.9750 - val_loss: 0.1126 - val_accuracy: 0.9333\n",
            "Epoch 180/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 0.9750 - val_loss: 0.1128 - val_accuracy: 0.9333\n",
            "Epoch 181/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.9750 - val_loss: 0.1121 - val_accuracy: 0.9667\n",
            "Epoch 182/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0450 - accuracy: 0.9750 - val_loss: 0.1130 - val_accuracy: 0.9333\n",
            "Epoch 183/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 0.9750 - val_loss: 0.1120 - val_accuracy: 0.9333\n",
            "Epoch 184/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 0.9750 - val_loss: 0.1114 - val_accuracy: 0.9667\n",
            "Epoch 185/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0444 - accuracy: 0.9750 - val_loss: 0.1115 - val_accuracy: 0.9667\n",
            "Epoch 186/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0443 - accuracy: 0.9750 - val_loss: 0.1120 - val_accuracy: 0.9333\n",
            "Epoch 187/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0441 - accuracy: 0.9750 - val_loss: 0.1117 - val_accuracy: 0.9333\n",
            "Epoch 188/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0440 - accuracy: 0.9750 - val_loss: 0.1114 - val_accuracy: 0.9333\n",
            "Epoch 189/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0439 - accuracy: 0.9750 - val_loss: 0.1111 - val_accuracy: 0.9333\n",
            "Epoch 190/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0440 - accuracy: 0.9750 - val_loss: 0.1114 - val_accuracy: 0.9333\n",
            "Epoch 191/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0438 - accuracy: 0.9750 - val_loss: 0.1111 - val_accuracy: 0.9333\n",
            "Epoch 192/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0436 - accuracy: 0.9750 - val_loss: 0.1106 - val_accuracy: 0.9333\n",
            "Epoch 193/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0436 - accuracy: 0.9750 - val_loss: 0.1107 - val_accuracy: 0.9333\n",
            "Epoch 194/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0436 - accuracy: 0.9750 - val_loss: 0.1106 - val_accuracy: 0.9333\n",
            "Epoch 195/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0436 - accuracy: 0.9750 - val_loss: 0.1106 - val_accuracy: 0.9333\n",
            "Epoch 196/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0435 - accuracy: 0.9750 - val_loss: 0.1096 - val_accuracy: 0.9667\n",
            "Epoch 197/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9750 - val_loss: 0.1106 - val_accuracy: 0.9333\n",
            "Epoch 198/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9750 - val_loss: 0.1099 - val_accuracy: 0.9333\n",
            "Epoch 199/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0433 - accuracy: 0.9750 - val_loss: 0.1096 - val_accuracy: 0.9667\n",
            "Epoch 200/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9750 - val_loss: 0.1092 - val_accuracy: 0.9667\n",
            "Epoch 201/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.9750 - val_loss: 0.1093 - val_accuracy: 0.9333\n",
            "Epoch 202/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0429 - accuracy: 0.9750 - val_loss: 0.1089 - val_accuracy: 0.9667\n",
            "Epoch 203/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0430 - accuracy: 0.9750 - val_loss: 0.1097 - val_accuracy: 0.9333\n",
            "Epoch 204/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0429 - accuracy: 0.9750 - val_loss: 0.1101 - val_accuracy: 0.9333\n",
            "Epoch 205/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.9750 - val_loss: 0.1098 - val_accuracy: 0.9333\n",
            "Epoch 206/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0427 - accuracy: 0.9750 - val_loss: 0.1085 - val_accuracy: 0.9333\n",
            "Epoch 207/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0425 - accuracy: 0.9750 - val_loss: 0.1087 - val_accuracy: 0.9333\n",
            "Epoch 208/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0425 - accuracy: 0.9750 - val_loss: 0.1086 - val_accuracy: 0.9333\n",
            "Epoch 209/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9750 - val_loss: 0.1081 - val_accuracy: 0.9667\n",
            "Epoch 210/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.9750 - val_loss: 0.1086 - val_accuracy: 0.9333\n",
            "Epoch 211/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.9750 - val_loss: 0.1087 - val_accuracy: 0.9333\n",
            "Epoch 212/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.9750 - val_loss: 0.1079 - val_accuracy: 0.9333\n",
            "Epoch 213/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0422 - accuracy: 0.9750 - val_loss: 0.1083 - val_accuracy: 0.9333\n",
            "Epoch 214/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0422 - accuracy: 0.9750 - val_loss: 0.1073 - val_accuracy: 0.9667\n",
            "Epoch 215/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0421 - accuracy: 0.9750 - val_loss: 0.1079 - val_accuracy: 0.9333\n",
            "Epoch 216/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9750 - val_loss: 0.1079 - val_accuracy: 0.9333\n",
            "Epoch 217/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9750 - val_loss: 0.1080 - val_accuracy: 0.9333\n",
            "Epoch 218/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0418 - accuracy: 0.9750 - val_loss: 0.1075 - val_accuracy: 0.9333\n",
            "Epoch 219/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0417 - accuracy: 0.9750 - val_loss: 0.1072 - val_accuracy: 0.9333\n",
            "Epoch 220/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.9750 - val_loss: 0.1077 - val_accuracy: 0.9333\n",
            "Epoch 221/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0415 - accuracy: 0.9750 - val_loss: 0.1072 - val_accuracy: 0.9333\n",
            "Epoch 222/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0417 - accuracy: 0.9750 - val_loss: 0.1064 - val_accuracy: 0.9333\n",
            "Epoch 223/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0415 - accuracy: 0.9750 - val_loss: 0.1059 - val_accuracy: 0.9667\n",
            "Epoch 224/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 0.9750 - val_loss: 0.1067 - val_accuracy: 0.9333\n",
            "Epoch 225/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 0.9750 - val_loss: 0.1074 - val_accuracy: 0.9333\n",
            "Epoch 226/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 0.9750 - val_loss: 0.1064 - val_accuracy: 0.9333\n",
            "Epoch 227/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0411 - accuracy: 0.9750 - val_loss: 0.1059 - val_accuracy: 0.9333\n",
            "Epoch 228/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0411 - accuracy: 0.9750 - val_loss: 0.1056 - val_accuracy: 0.9333\n",
            "Epoch 229/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0412 - accuracy: 0.9750 - val_loss: 0.1060 - val_accuracy: 0.9333\n",
            "Epoch 230/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0411 - accuracy: 0.9750 - val_loss: 0.1062 - val_accuracy: 0.9333\n",
            "Epoch 231/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0413 - accuracy: 0.9750 - val_loss: 0.1051 - val_accuracy: 0.9667\n",
            "Epoch 232/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0409 - accuracy: 0.9750 - val_loss: 0.1047 - val_accuracy: 0.9667\n",
            "Epoch 233/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0412 - accuracy: 0.9750 - val_loss: 0.1059 - val_accuracy: 0.9333\n",
            "Epoch 234/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0408 - accuracy: 0.9750 - val_loss: 0.1051 - val_accuracy: 0.9333\n",
            "Epoch 235/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9750 - val_loss: 0.1048 - val_accuracy: 0.9333\n",
            "Epoch 236/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0408 - accuracy: 0.9750 - val_loss: 0.1047 - val_accuracy: 0.9333\n",
            "Epoch 237/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0406 - accuracy: 0.9750 - val_loss: 0.1048 - val_accuracy: 0.9333\n",
            "Epoch 238/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0406 - accuracy: 0.9750 - val_loss: 0.1049 - val_accuracy: 0.9333\n",
            "Epoch 239/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0405 - accuracy: 0.9750 - val_loss: 0.1046 - val_accuracy: 0.9333\n",
            "Epoch 240/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0409 - accuracy: 0.9750 - val_loss: 0.1054 - val_accuracy: 0.9333\n",
            "Epoch 241/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0404 - accuracy: 0.9750 - val_loss: 0.1051 - val_accuracy: 0.9333\n",
            "Epoch 242/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.9750 - val_loss: 0.1054 - val_accuracy: 0.9333\n",
            "Epoch 243/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0402 - accuracy: 0.9750 - val_loss: 0.1045 - val_accuracy: 0.9333\n",
            "Epoch 244/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9750 - val_loss: 0.1041 - val_accuracy: 0.9333\n",
            "Epoch 245/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0403 - accuracy: 0.9750 - val_loss: 0.1041 - val_accuracy: 0.9333\n",
            "Epoch 246/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9750 - val_loss: 0.1038 - val_accuracy: 0.9333\n",
            "Epoch 247/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0404 - accuracy: 0.9750 - val_loss: 0.1042 - val_accuracy: 0.9333\n",
            "Epoch 248/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0404 - accuracy: 0.9750 - val_loss: 0.1028 - val_accuracy: 0.9667\n",
            "Epoch 249/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0406 - accuracy: 0.9750 - val_loss: 0.1035 - val_accuracy: 0.9333\n",
            "Epoch 250/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 0.9750 - val_loss: 0.1034 - val_accuracy: 0.9333\n",
            "Epoch 251/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0399 - accuracy: 0.9750 - val_loss: 0.1031 - val_accuracy: 0.9333\n",
            "Epoch 252/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0399 - accuracy: 0.9750 - val_loss: 0.1030 - val_accuracy: 0.9333\n",
            "Epoch 253/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 0.9750 - val_loss: 0.1023 - val_accuracy: 0.9667\n",
            "Epoch 254/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0398 - accuracy: 0.9750 - val_loss: 0.1028 - val_accuracy: 0.9333\n",
            "Epoch 255/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0401 - accuracy: 0.9750 - val_loss: 0.1017 - val_accuracy: 0.9667\n",
            "Epoch 256/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9750 - val_loss: 0.1026 - val_accuracy: 0.9333\n",
            "Epoch 257/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9750 - val_loss: 0.1027 - val_accuracy: 0.9333\n",
            "Epoch 258/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0395 - accuracy: 0.9750 - val_loss: 0.1025 - val_accuracy: 0.9333\n",
            "Epoch 259/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0395 - accuracy: 0.9750 - val_loss: 0.1020 - val_accuracy: 0.9333\n",
            "Epoch 260/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0395 - accuracy: 0.9750 - val_loss: 0.1021 - val_accuracy: 0.9333\n",
            "Epoch 261/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 0.9750 - val_loss: 0.1031 - val_accuracy: 0.9333\n",
            "Epoch 262/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9750 - val_loss: 0.1026 - val_accuracy: 0.9333\n",
            "Epoch 263/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0394 - accuracy: 0.9750 - val_loss: 0.1017 - val_accuracy: 0.9333\n",
            "Epoch 264/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 0.9750 - val_loss: 0.1016 - val_accuracy: 0.9333\n",
            "Epoch 265/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 0.9750 - val_loss: 0.1017 - val_accuracy: 0.9333\n",
            "Epoch 266/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0394 - accuracy: 0.9750 - val_loss: 0.1008 - val_accuracy: 0.9667\n",
            "Epoch 267/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.9750 - val_loss: 0.1011 - val_accuracy: 0.9333\n",
            "Epoch 268/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 0.9750 - val_loss: 0.1026 - val_accuracy: 0.9333\n",
            "Epoch 269/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0390 - accuracy: 0.9750 - val_loss: 0.1021 - val_accuracy: 0.9333\n",
            "Epoch 270/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0391 - accuracy: 0.9750 - val_loss: 0.1010 - val_accuracy: 0.9333\n",
            "Epoch 271/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0392 - accuracy: 0.9750 - val_loss: 0.1005 - val_accuracy: 0.9667\n",
            "Epoch 272/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 0.9750 - val_loss: 0.1012 - val_accuracy: 0.9333\n",
            "Epoch 273/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.9750 - val_loss: 0.1015 - val_accuracy: 0.9333\n",
            "Epoch 274/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0391 - accuracy: 0.9750 - val_loss: 0.1020 - val_accuracy: 0.9333\n",
            "Epoch 275/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0389 - accuracy: 0.9750 - val_loss: 0.1011 - val_accuracy: 0.9333\n",
            "Epoch 276/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0388 - accuracy: 0.9750 - val_loss: 0.1010 - val_accuracy: 0.9333\n",
            "Epoch 277/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.9750 - val_loss: 0.1006 - val_accuracy: 0.9333\n",
            "Epoch 278/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.9750 - val_loss: 0.1001 - val_accuracy: 0.9333\n",
            "Epoch 279/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9750 - val_loss: 0.0997 - val_accuracy: 0.9333\n",
            "Epoch 280/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.9750 - val_loss: 0.0999 - val_accuracy: 0.9333\n",
            "Epoch 281/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0386 - accuracy: 0.9750 - val_loss: 0.1003 - val_accuracy: 0.9333\n",
            "Epoch 282/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9750 - val_loss: 0.1005 - val_accuracy: 0.9333\n",
            "Epoch 283/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0390 - accuracy: 0.9750 - val_loss: 0.0992 - val_accuracy: 0.9667\n",
            "Epoch 284/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0384 - accuracy: 0.9750 - val_loss: 0.0997 - val_accuracy: 0.9333\n",
            "Epoch 285/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0391 - accuracy: 0.9750 - val_loss: 0.1010 - val_accuracy: 0.9333\n",
            "Epoch 286/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0384 - accuracy: 0.9833 - val_loss: 0.1007 - val_accuracy: 0.9333\n",
            "Epoch 287/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 0.9750 - val_loss: 0.0999 - val_accuracy: 0.9333\n",
            "Epoch 288/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0387 - accuracy: 0.9750 - val_loss: 0.1003 - val_accuracy: 0.9333\n",
            "Epoch 289/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0384 - accuracy: 0.9750 - val_loss: 0.0994 - val_accuracy: 0.9333\n",
            "Epoch 290/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0382 - accuracy: 0.9750 - val_loss: 0.0992 - val_accuracy: 0.9333\n",
            "Epoch 291/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0383 - accuracy: 0.9750 - val_loss: 0.0996 - val_accuracy: 0.9333\n",
            "Epoch 292/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0382 - accuracy: 0.9750 - val_loss: 0.0998 - val_accuracy: 0.9333\n",
            "Epoch 293/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0386 - accuracy: 0.9750 - val_loss: 0.1004 - val_accuracy: 0.9333\n",
            "Epoch 294/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9750 - val_loss: 0.0987 - val_accuracy: 0.9333\n",
            "Epoch 295/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9750 - val_loss: 0.0992 - val_accuracy: 0.9333\n",
            "Epoch 296/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0385 - accuracy: 0.9750 - val_loss: 0.0978 - val_accuracy: 0.9667\n",
            "Epoch 297/300\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0381 - accuracy: 0.9750 - val_loss: 0.0983 - val_accuracy: 0.9333\n",
            "Epoch 298/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0382 - accuracy: 0.9750 - val_loss: 0.0992 - val_accuracy: 0.9333\n",
            "Epoch 299/300\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.9750 - val_loss: 0.0992 - val_accuracy: 0.9333\n",
            "Epoch 300/300\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 0.9750 - val_loss: 0.0988 - val_accuracy: 0.9333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgG4jjt8y9ur"
      },
      "source": [
        "Now we can see that with the increased epoch the model has performed well where train accuracy is 97.5% and test Accuracy is 93.33%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5VIoORv0FAo"
      },
      "source": [
        "And Training loss is 0.0379 and test loss is 0.0988"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX_fJzsnQTMy",
        "outputId": "8feccd63-ef59-48a2-99ff-ac0a179f10cb"
      },
      "source": [
        "print(model.get_weights())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[-0.6855354 , -0.08726685,  0.23358962,  0.4790395 , -0.26671037,\n",
            "        -0.44769892, -0.37546718, -0.31803223],\n",
            "       [ 0.55617476, -0.3265764 , -0.7435184 , -0.9415272 ,  0.2027133 ,\n",
            "        -0.19971107,  0.63361526,  0.34650287],\n",
            "       [-0.60330564,  1.3442799 ,  1.4192328 ,  0.12344559, -1.4238826 ,\n",
            "         1.4996015 , -0.05230531, -0.94398457],\n",
            "       [-0.90983206,  0.825686  ,  0.235517  , -0.65045214, -0.39128104,\n",
            "         1.3600217 , -0.9706438 , -1.5388937 ]], dtype=float32), array([ 0.06132568, -0.7292609 ,  0.0114885 ,  1.3697991 , -0.0940122 ,\n",
            "       -0.56245947,  1.0006864 ,  0.01210283], dtype=float32), array([[ 0.53316706, -1.2553033 , -0.82151616],\n",
            "       [-0.3040513 , -1.2232887 ,  1.5988466 ],\n",
            "       [-1.190954  , -0.05161718,  0.81615734],\n",
            "       [-0.7296791 ,  1.4933326 , -0.5874699 ],\n",
            "       [ 0.5829529 , -1.5601752 , -0.7427205 ],\n",
            "       [-1.3153737 , -1.2150459 ,  0.76173824],\n",
            "       [ 0.2330446 ,  0.7730677 , -2.321918  ],\n",
            "       [ 0.04485407, -1.5342324 , -0.5370451 ]], dtype=float32), array([-0.9570237 ,  1.0179038 , -0.75544834], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgtzL_WBzTTF"
      },
      "source": [
        "The above are the further adjusted weighs and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaYHwqfKQM8N",
        "outputId": "526f379e-091e-408e-8c4a-60afbd03b407"
      },
      "source": [
        "score = model.evaluate(x_test,y_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0988 - accuracy: 0.9333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ2BH0l70Uop"
      },
      "source": [
        "**Step 8: Predicting the test dataset to check how well the model is predicting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJoQPlg9Qr1R",
        "outputId": "dc76c371-491c-4043-902a-fb5c23e3179b"
      },
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9999917e-01, 8.6015848e-07, 3.1382870e-09],\n",
              "       [1.8734207e-05, 1.4298657e-03, 9.9855143e-01],\n",
              "       [5.4270306e-05, 1.8819887e-02, 9.8112583e-01],\n",
              "       [2.4743858e-04, 2.3762903e-01, 7.6212353e-01],\n",
              "       [9.2410803e-05, 2.0224653e-01, 7.9766107e-01],\n",
              "       [1.3006042e-04, 9.9931383e-01, 5.5612868e-04],\n",
              "       [5.4098992e-03, 3.3604842e-01, 6.5854168e-01],\n",
              "       [1.3263045e-03, 9.8823208e-01, 1.0441653e-02],\n",
              "       [8.5862657e-06, 5.2062625e-01, 4.7936523e-01],\n",
              "       [8.4062293e-03, 9.4966483e-01, 4.1928995e-02],\n",
              "       [5.0962451e-05, 7.7267295e-01, 2.2727610e-01],\n",
              "       [9.9999702e-01, 2.9756839e-06, 8.4245606e-09],\n",
              "       [9.9995935e-01, 4.0628118e-05, 4.4621611e-08],\n",
              "       [3.2798711e-05, 5.6099007e-03, 9.9435729e-01],\n",
              "       [8.8795420e-04, 1.8461736e-01, 8.1449473e-01],\n",
              "       [9.9994302e-01, 5.6821249e-05, 7.7643847e-08],\n",
              "       [1.4582011e-04, 9.7566175e-01, 2.4192438e-02],\n",
              "       [7.5758598e-04, 9.9580294e-01, 3.4394243e-03],\n",
              "       [1.6425833e-05, 7.2944548e-04, 9.9925417e-01],\n",
              "       [2.8542988e-04, 9.9674571e-01, 2.9688857e-03],\n",
              "       [2.0505409e-03, 9.9733967e-01, 6.0978561e-04],\n",
              "       [8.3911000e-06, 4.3287417e-03, 9.9566287e-01],\n",
              "       [3.7007614e-03, 9.6000475e-01, 3.6294468e-02],\n",
              "       [7.0502580e-07, 7.0667909e-03, 9.9293256e-01],\n",
              "       [9.9989867e-01, 1.0108769e-04, 2.0292504e-07],\n",
              "       [9.9999011e-01, 9.8701212e-06, 2.3259730e-08],\n",
              "       [9.9999988e-01, 7.0127022e-08, 1.2511971e-10],\n",
              "       [9.9995315e-01, 4.6796824e-05, 5.3923692e-08],\n",
              "       [9.9997973e-01, 2.0135874e-05, 6.8475288e-08],\n",
              "       [9.9999952e-01, 5.3145692e-07, 2.5606928e-09]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v78vcxsk0lqm"
      },
      "source": [
        "As the predicted values and y_test values are in probabilties of each classess, using numpy library's argmax function to return only the highest values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF8bJxWJQy-o"
      },
      "source": [
        "y_pred_class = np.argmax(y_pred,axis=1)\n",
        "y_test_class = np.argmax(y_test,axis=1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ7gQqj71DNP"
      },
      "source": [
        "**Step 9: Importing classification report in order to calculate the F1 score, and importing confusion matrix to calculate the true positive and true negatives from sklearn metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIDo3fliQziI"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHvEBON2Q1gF",
        "outputId": "233c2b1f-b062-4761-cb47-a50a78dcf693"
      },
      "source": [
        "cm = confusion_matrix(y_test_class,y_pred_class)\n",
        "print(cm)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10  0  0]\n",
            " [ 0  9  1]\n",
            " [ 0  1  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tSzuSNK1jyE"
      },
      "source": [
        "**From the above confusion matrics it is shown that model predicted 28 values correctly out of 30 values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMnVrP4VQ3CF",
        "outputId": "abee525d-4fe8-4466-9534-172179bead6d"
      },
      "source": [
        "cl = classification_report(y_test_class,y_pred_class)\n",
        "print(cl)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.90      0.90      0.90        10\n",
            "           2       0.90      0.90      0.90        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.93      0.93      0.93        30\n",
            "weighted avg       0.93      0.93      0.93        30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZblJZ6pZ14Ac"
      },
      "source": [
        "**And the F1 Score for : class 0 is 100% ,class 1 is 90%, class 2 is 90%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4lZVPWAQ5Kb",
        "outputId": "fad19f2c-d063-4a40-a114-6962485a3f7c"
      },
      "source": [
        "NN_model_2.history.keys()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1U-0i1Q2RZd"
      },
      "source": [
        "**Using history function to retrive the train loss for each epochs in order to plot it using Matplotlib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "__Dy0XXjRbfV",
        "outputId": "16005945-bb82-4629-b6d1-4ae0b15c5056"
      },
      "source": [
        "plt.plot(NN_model_2.history[\"loss\"])\n",
        "plt.title(\"Loss VS Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend([\"train_loss\"])\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgqbA7AZ22pL"
      },
      "source": [
        "**Result : The train loss was minimized exponentialy at the begining and gradually toward the end through out epochs.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te-5-98jR8o-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}